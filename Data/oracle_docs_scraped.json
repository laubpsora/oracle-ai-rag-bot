[
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/overview.htm#use-cases",
    "title": "Overview of Generative AI Service",
    "text": "Overview of Generative AI Service Generative AI is a fully managed Oracle Cloud Infrastructure service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases, including chat, text generation, summarization, and creating text embeddings. Use the playground, the API, or the CLI to try out the ready-to-use pretrained models or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. The OCI Generative AI service includes the following foundational models for chat, rerank, and text embeddings. Chat Ask questions and get conversational responses through an AI chatbot. Rerank Input a query and a list of texts and get an ordered array with each text assigned a relevance score. The relevance score is how the model ranks the documents, that's, how well each text matches the query. Embedding Convert text to vector embeddings to use in applications for semantic searches, recommender systems, text classification, or text clustering. Using the Pretrained Foundational Models To get started, use the playground to try the pretrained foundational models. Run your prompts, adjust the parameters, update your prompts, and rerun the models until you're happy with the results. Then get the code from the Console and copy the code into your applications. Fine-Tuning the Pretrained Models You can create a copy of a pretrained foundational model, add your own training dataset, and let the OCI Generative AI service fine-tune the model for you. OCI Generative AI uses dedicated AI clusters specially sized for fine-tuning. These clusters belong only to your tenancy. After your model is fine-tuned, you create an endpoint for the custom model and host that model on a dedicated AI cluster that's designed for hosting. When you create the hosting cluster, select the correct pretrained model from which the fine-tuned model is derived from. Use Cases Use the OCI Generative AI service for the following types of use cases. Text Generation Use the pretrained chat models or text generation models to create text for any purpose, for example: Pitch for a new product Slogan for a marketing campaign Sales email to a client Social media post Job description Title for an article Conversation You can ask questions in natural language and optionally submit text such as documents, emails, and product reviews to the LLM and the LLM reasons over the text and provides intelligent answers. Data Extraction Extract specific pieces of data from text, for example: Extract applicant information from an application written in free-form text. Extract dates or sums from a contract. Extract insights or trends from data tables. Summarization Generate executive summaries for documents that are too long to read, or summarize any type of text, for example: Documents Contracts Emails Articles Blog posts Product reviews Social media posts Classification Classify text into predefined categories, for example: Given a list of support tickets, classify them by the department that should handle them. Given a list of sectors and company names, classify the companies by their respective sectors. Style Transfer Change the style or tone of text, for example: Rewrite any text in a different style, format (list or paragraphs), or tone. Rephrase text. Suggest grammatical improvements. Semantic Similarity Evaluate several inputs based on how similar their meaning is, for example: Evaluate a list of questions sent to a support system to extract the most relevant answer given to similar questions in the past when a new question comes in. Replace keyword-based searches with semantic searches to improve search results relevance. Regions with Generative AI Oracle hosts its OCI services in regions and availability domains. A region is a localized geographic area, and an availability domain is one or more data centers in that region. OCI Generative AI is hosted in the following regions: Important Some regions don't offer all the models. See the region for each pretrained model to find out which models are available in a region near you. Region Name Location Region Identifier Region Key Brazil East (Sao Paulo) Sao Paulo sa-saopaulo-1 GRU Germany Central (Frankfurt) Frankfurt eu-frankfurt-1 FRA India South (Hyderabad) (New) Hyderabad ap-hyderabad-1 HYD Japan Central (Osaka) Osaka ap-osaka-1 KIX UAE East (Dubai) Dubai me-dubai-1 DXB UK South (London) London uk-london-1 LHR US Midwest (Chicago) Chicago us-chicago-1 ORD See About Regions and Availability Domains for a complete list of available OCI regions, along with associated locations, region identifiers, region keys, and availability domains. Pretrained Models with Cross-Region Calls See the column below, titled \"Destination Region\" for the OCI commercial region offering pre-trained models for the Generative AI Service where a cross-region call is made between the \"Hosting Region\" and the \"Destination Region\". For example, when a user enters an inference request to use the xAI Grok 3 model in Chicago, Generative AI service in Chicago makes a request to this model hosted in Salt Lake City, and returns the model's response back to Chicago where the user inference request came from. For a list of pretrained models in OCI Generative AI and their regions, see Pretrained Foundational Models in Generative AI. Area Hosting Region: OCI Generative AI Service calls into this region to access the hosted model and returns the model's response back to the Destination Region Destination Region:OCI region offering OCI Generative AI Service where user sends requests to a pretrained model Pretrained Models: Models in OCI Generative AI Service with cross-region calls U.S. Salt Lake City ORD (Chicago) xAI Grok 3 xAI Grok 3 Mini xAI Grok 3 Fast xAI Grok 3 Mini Fast Services that Call into the Generative AI Service Currently, the Oracle Cloud Infrastructure (OCI) Generative AI Service is not offered in every OCI commercial region. See the column below titled “Destination Region:” for a list of OCI commercial regions offering the OCI Generative AI Service. There are OCI services which document (in the “Oracle PaaS and IaaS Universal Credits Service Descriptions”) that they make calls into the OCI Generative AI Service. For a given call into the OCI Generative AI Service, if the Calling Region and Destination Region are not the same, then a cross-region call will be made. Area Calling Regions:OCI regions from which (cross-region) calls can be made to the OCI Generative AI Service Destination Region:OCI region offering OCI Generative AI Service Brazil GRU, VCP GRU EU AMS, MRS, LIN, ARN, CDG, MAD, ZRH, FRA FRA India HYD HYD Japan KIX, NRT KIX UAE DXB DXB UK CWL, LHR LHR U.S. PHX, IAD, SJC, ORD ORD Accessing Generative AI in the Console Sign in to the Console by using a supported browser. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI.",
    "word_count": 1130,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/overview.htm#overview",
    "title": "Overview of Generative AI Service",
    "text": "Overview of Generative AI Service Generative AI is a fully managed Oracle Cloud Infrastructure service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases, including chat, text generation, summarization, and creating text embeddings. Use the playground, the API, or the CLI to try out the ready-to-use pretrained models or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. The OCI Generative AI service includes the following foundational models for chat, rerank, and text embeddings. Chat Ask questions and get conversational responses through an AI chatbot. Rerank Input a query and a list of texts and get an ordered array with each text assigned a relevance score. The relevance score is how the model ranks the documents, that's, how well each text matches the query. Embedding Convert text to vector embeddings to use in applications for semantic searches, recommender systems, text classification, or text clustering. Using the Pretrained Foundational Models To get started, use the playground to try the pretrained foundational models. Run your prompts, adjust the parameters, update your prompts, and rerun the models until you're happy with the results. Then get the code from the Console and copy the code into your applications. Fine-Tuning the Pretrained Models You can create a copy of a pretrained foundational model, add your own training dataset, and let the OCI Generative AI service fine-tune the model for you. OCI Generative AI uses dedicated AI clusters specially sized for fine-tuning. These clusters belong only to your tenancy. After your model is fine-tuned, you create an endpoint for the custom model and host that model on a dedicated AI cluster that's designed for hosting. When you create the hosting cluster, select the correct pretrained model from which the fine-tuned model is derived from. Use Cases Use the OCI Generative AI service for the following types of use cases. Text Generation Use the pretrained chat models or text generation models to create text for any purpose, for example: Pitch for a new product Slogan for a marketing campaign Sales email to a client Social media post Job description Title for an article Conversation You can ask questions in natural language and optionally submit text such as documents, emails, and product reviews to the LLM and the LLM reasons over the text and provides intelligent answers. Data Extraction Extract specific pieces of data from text, for example: Extract applicant information from an application written in free-form text. Extract dates or sums from a contract. Extract insights or trends from data tables. Summarization Generate executive summaries for documents that are too long to read, or summarize any type of text, for example: Documents Contracts Emails Articles Blog posts Product reviews Social media posts Classification Classify text into predefined categories, for example: Given a list of support tickets, classify them by the department that should handle them. Given a list of sectors and company names, classify the companies by their respective sectors. Style Transfer Change the style or tone of text, for example: Rewrite any text in a different style, format (list or paragraphs), or tone. Rephrase text. Suggest grammatical improvements. Semantic Similarity Evaluate several inputs based on how similar their meaning is, for example: Evaluate a list of questions sent to a support system to extract the most relevant answer given to similar questions in the past when a new question comes in. Replace keyword-based searches with semantic searches to improve search results relevance. Regions with Generative AI Oracle hosts its OCI services in regions and availability domains. A region is a localized geographic area, and an availability domain is one or more data centers in that region. OCI Generative AI is hosted in the following regions: Important Some regions don't offer all the models. See the region for each pretrained model to find out which models are available in a region near you. Region Name Location Region Identifier Region Key Brazil East (Sao Paulo) Sao Paulo sa-saopaulo-1 GRU Germany Central (Frankfurt) Frankfurt eu-frankfurt-1 FRA India South (Hyderabad) (New) Hyderabad ap-hyderabad-1 HYD Japan Central (Osaka) Osaka ap-osaka-1 KIX UAE East (Dubai) Dubai me-dubai-1 DXB UK South (London) London uk-london-1 LHR US Midwest (Chicago) Chicago us-chicago-1 ORD See About Regions and Availability Domains for a complete list of available OCI regions, along with associated locations, region identifiers, region keys, and availability domains. Pretrained Models with Cross-Region Calls See the column below, titled \"Destination Region\" for the OCI commercial region offering pre-trained models for the Generative AI Service where a cross-region call is made between the \"Hosting Region\" and the \"Destination Region\". For example, when a user enters an inference request to use the xAI Grok 3 model in Chicago, Generative AI service in Chicago makes a request to this model hosted in Salt Lake City, and returns the model's response back to Chicago where the user inference request came from. For a list of pretrained models in OCI Generative AI and their regions, see Pretrained Foundational Models in Generative AI. Area Hosting Region: OCI Generative AI Service calls into this region to access the hosted model and returns the model's response back to the Destination Region Destination Region:OCI region offering OCI Generative AI Service where user sends requests to a pretrained model Pretrained Models: Models in OCI Generative AI Service with cross-region calls U.S. Salt Lake City ORD (Chicago) xAI Grok 3 xAI Grok 3 Mini xAI Grok 3 Fast xAI Grok 3 Mini Fast Services that Call into the Generative AI Service Currently, the Oracle Cloud Infrastructure (OCI) Generative AI Service is not offered in every OCI commercial region. See the column below titled “Destination Region:” for a list of OCI commercial regions offering the OCI Generative AI Service. There are OCI services which document (in the “Oracle PaaS and IaaS Universal Credits Service Descriptions”) that they make calls into the OCI Generative AI Service. For a given call into the OCI Generative AI Service, if the Calling Region and Destination Region are not the same, then a cross-region call will be made. Area Calling Regions:OCI regions from which (cross-region) calls can be made to the OCI Generative AI Service Destination Region:OCI region offering OCI Generative AI Service Brazil GRU, VCP GRU EU AMS, MRS, LIN, ARN, CDG, MAD, ZRH, FRA FRA India HYD HYD Japan KIX, NRT KIX UAE DXB DXB UK CWL, LHR LHR U.S. PHX, IAD, SJC, ORD ORD Accessing Generative AI in the Console Sign in to the Console by using a supported browser. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI.",
    "word_count": 1130,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/concepts.htm#concepts",
    "title": "Concepts for Generative AI",
    "text": "Concepts for Generative AI To help you underatand OCI Generative AI, review some concepts and terms related to the service. Generative AI Model An AI model trained on large amounts of data which takes inputs that it hasn't seen before and generates new content. Retrieval-Augmented Generation (RAG) A program that retrieves data from given sources and augments large language model (LLM) responses with the given information to generate grounded responses. Prompts and Prompt Engineering Prompts Strings of text in natural language used to instruct or extract information from a large language model. For example, What is the summer solstice? Write a poem about trees swaying in the breeze. Rewrite the previous text in a lighter tone. Prompt Engineering The iterative process of crafting specific requests in natural language for extracting optimized prompts from a large language model (LLM). Based on the exact language used, the prompt engineer can guide the LLM to provide better or different outputs. Inference The ability of a large language model (LLM) to generate a response based on instructions and context provided by the user in the prompt. An LLM can generate new data, make predictions, or draw conclusions based on its learned patterns and relationships in the training data, without having been explicitly programmed. Inference is a key feature of natural language processing (NLP) tasks such as question answering, summarizing text, and translating. You can use the foundational models in Generative AI for inference. Streaming Generation of content by a large language model (LLM) where the user can see the tokens being generated one at a time instead of waiting for a complete response to be generated before returning the response to the user. Embedding A numerical representation that has the property of preserving the meaning of a piece of text. This text can be a phrase, a sentence, or one or more paragraphs. The Generative AI embedding models transform each phrase, sentence, or paragraph that you input, into an array with 384 or 1024 numbers, depending on the embedding model that you choose. You can use these embeddings for finding similarity in phrases that are similar in context or category. Embeddings are typically stored in a vector database. Embeddings are mostly used for semantic searches where the search function focuses on the meaning of the text that it's searching through rather than finding results based on keywords. To create the embeddings, you can input phrases in English and other languages. Playground An interface in the Oracle Cloud Console for exploring the hosted pretrained and custom models without writing a single line of code. Use the playground to test your use cases and refine prompts and parameters. When you're happy with the results, copy the generated code or use the model's endpoint to integrate Generative AI into your applications. Custom Model A model that you create by using a pretrained model as a base and using your own dataset to fine-tune that model. Tokens A token is a word, part of a word, or a punctuation. For example, apple is one token and friendship is two tokens (friend and ship), and don’t is two tokens (don and ‘t). When you run a model in the playground, you can set the maximum number of output tokens. Estimate four characters per token. Temperature The level of randomness used to generate the output text. To generate a similar output for a prompt every time that you run that prompt, use 0. To generate a random new text for that prompt, increase the temperature. Tip Start with the temperature set to 0 and increase the temperature as you regenerate the prompts to refine the output. High temperatures can introduce hallucinations and factually incorrect information. To aim to get the same result for repeated requests, use the seed parameter. Top k A sampling method in which the model chooses the next token randomly from the top k most likely tokens. A higher value for k generates more random output, which makes the output text sound more natural. The default value for k is 0 for command models and -1 for Llama models, which means that the models should consider all tokens and not use this method. Top p A sampling method that controls the cumulative probability of the top tokens to consider for the next token. Assign p a decimal number between 0 and 1 for the probability. For example, enter 0.75 for the top 75 percent to be considered. Set p to 1 to consider all tokens. Frequency Penalty A penalty that is assigned to a token when that token appears frequently. High penalties encourage fewer repeated tokens and produce a more random output. Presence Penalty A penalty that is assigned to each token when it appears in the output to encourage generating outputs with tokens that haven't been used. Likelihood In the output of a large language model (LLM), how likely it is for a token to follow the current generated token. When an LLM generates a new token for the output text, a likelihood is assigned to all tokens, where tokens with higher likelihoods are more likely to follow the current token. For example, it's more likely that the word favorite is followed by the word food or book rather than the word zebra. Likelihood is defined by a number between -15 and 0 and the more negative the number, the less likely it is that the token follows the current token. Preamble An initial context or guiding message for a chat model. When you don't give a preamble to a chat model, the default preamble for that model is used. The default preamble for the cohere.command-r-plus and cohere.command-r-16k models is: You are Command. You are an extremely capable large language model built by Cohere. You are given instructions programmatically via an API that you follow to the best of your ability. It's optional to give a preamble. If want to use your own preamble, for best results, give the model context, instructions, and a conversation style. Here are some examples: You are a seasoned marketing professional with a deep understanding of consumer behavior and market trends. Answer with a friendly and informative tone, sharing industry insights and best practices. You are a travel advisor that focuses on fun activities. Answer with sense of humor and a pirate tone. Note You can also include a preamble in a chat conversation and directly ask the model to answer in a certain way. For example, \"Answer the following question in a marketing tone. Where's the best place to go sailing?\" Model Endpoint A designated point on a dedicated AI cluster where a large language model (LLM) can accept user requests and send back responses such as the model's generated text. In OCI Generative AI, you can create endpoints for ready-to-use pretrained models and custom models. Those endpoints are listed in the playground for testing the models. You can also reference those endpoints in applications. Content Moderation A feature that removes toxic, violent, abusive, derogatory, hateful, threatening, insulting, and harassing phrases from generated responses in large language models (LLMs). In OCI Generative AI, content moderation is divided into the following four categories. Hate and harassment, such as identity attacks, insults, threats of violence, and sexual aggression Self-inflicted harm, such as self-harm and eating-disorder promotion Ideological harm, such as extremism, terrorism, organized crime, and misinformation Exploitation, such as scams and sexual abuse By default, OCI Generative AI doesn't add a content moderation layer on top of the ready-to-use pretrained models. However, pretrained models have some level of content moderation that filter the output responses. To incorporate content moderation into models, you must enable content moderation when creating an endpoint for a pretrained or a fine-tuned model. See Creating an Endpoint in Generative AI. Dedicated AI Clusters Compute resources that you can use for fine-tuning custom models or for hosting endpoints for pretrained and custom models. The clusters are dedicated to your models and not shared with other customers. Retired and Deprecated Models Retirement When a model is retired, it's no longer available for use in the Generative AI service. Deprecation When a model is deprecated it remains available in the Generative AI service, but will have a defined amount of time that it can be used before it's retired. For more information, see Retiring the Models.",
    "word_count": 1385,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/overview.htm#regions",
    "title": "Overview of Generative AI Service",
    "text": "Overview of Generative AI Service Generative AI is a fully managed Oracle Cloud Infrastructure service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases, including chat, text generation, summarization, and creating text embeddings. Use the playground, the API, or the CLI to try out the ready-to-use pretrained models or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. The OCI Generative AI service includes the following foundational models for chat, rerank, and text embeddings. Chat Ask questions and get conversational responses through an AI chatbot. Rerank Input a query and a list of texts and get an ordered array with each text assigned a relevance score. The relevance score is how the model ranks the documents, that's, how well each text matches the query. Embedding Convert text to vector embeddings to use in applications for semantic searches, recommender systems, text classification, or text clustering. Using the Pretrained Foundational Models To get started, use the playground to try the pretrained foundational models. Run your prompts, adjust the parameters, update your prompts, and rerun the models until you're happy with the results. Then get the code from the Console and copy the code into your applications. Fine-Tuning the Pretrained Models You can create a copy of a pretrained foundational model, add your own training dataset, and let the OCI Generative AI service fine-tune the model for you. OCI Generative AI uses dedicated AI clusters specially sized for fine-tuning. These clusters belong only to your tenancy. After your model is fine-tuned, you create an endpoint for the custom model and host that model on a dedicated AI cluster that's designed for hosting. When you create the hosting cluster, select the correct pretrained model from which the fine-tuned model is derived from. Use Cases Use the OCI Generative AI service for the following types of use cases. Text Generation Use the pretrained chat models or text generation models to create text for any purpose, for example: Pitch for a new product Slogan for a marketing campaign Sales email to a client Social media post Job description Title for an article Conversation You can ask questions in natural language and optionally submit text such as documents, emails, and product reviews to the LLM and the LLM reasons over the text and provides intelligent answers. Data Extraction Extract specific pieces of data from text, for example: Extract applicant information from an application written in free-form text. Extract dates or sums from a contract. Extract insights or trends from data tables. Summarization Generate executive summaries for documents that are too long to read, or summarize any type of text, for example: Documents Contracts Emails Articles Blog posts Product reviews Social media posts Classification Classify text into predefined categories, for example: Given a list of support tickets, classify them by the department that should handle them. Given a list of sectors and company names, classify the companies by their respective sectors. Style Transfer Change the style or tone of text, for example: Rewrite any text in a different style, format (list or paragraphs), or tone. Rephrase text. Suggest grammatical improvements. Semantic Similarity Evaluate several inputs based on how similar their meaning is, for example: Evaluate a list of questions sent to a support system to extract the most relevant answer given to similar questions in the past when a new question comes in. Replace keyword-based searches with semantic searches to improve search results relevance. Regions with Generative AI Oracle hosts its OCI services in regions and availability domains. A region is a localized geographic area, and an availability domain is one or more data centers in that region. OCI Generative AI is hosted in the following regions: Important Some regions don't offer all the models. See the region for each pretrained model to find out which models are available in a region near you. Region Name Location Region Identifier Region Key Brazil East (Sao Paulo) Sao Paulo sa-saopaulo-1 GRU Germany Central (Frankfurt) Frankfurt eu-frankfurt-1 FRA India South (Hyderabad) (New) Hyderabad ap-hyderabad-1 HYD Japan Central (Osaka) Osaka ap-osaka-1 KIX UAE East (Dubai) Dubai me-dubai-1 DXB UK South (London) London uk-london-1 LHR US Midwest (Chicago) Chicago us-chicago-1 ORD See About Regions and Availability Domains for a complete list of available OCI regions, along with associated locations, region identifiers, region keys, and availability domains. Pretrained Models with Cross-Region Calls See the column below, titled \"Destination Region\" for the OCI commercial region offering pre-trained models for the Generative AI Service where a cross-region call is made between the \"Hosting Region\" and the \"Destination Region\". For example, when a user enters an inference request to use the xAI Grok 3 model in Chicago, Generative AI service in Chicago makes a request to this model hosted in Salt Lake City, and returns the model's response back to Chicago where the user inference request came from. For a list of pretrained models in OCI Generative AI and their regions, see Pretrained Foundational Models in Generative AI. Area Hosting Region: OCI Generative AI Service calls into this region to access the hosted model and returns the model's response back to the Destination Region Destination Region:OCI region offering OCI Generative AI Service where user sends requests to a pretrained model Pretrained Models: Models in OCI Generative AI Service with cross-region calls U.S. Salt Lake City ORD (Chicago) xAI Grok 3 xAI Grok 3 Mini xAI Grok 3 Fast xAI Grok 3 Mini Fast Services that Call into the Generative AI Service Currently, the Oracle Cloud Infrastructure (OCI) Generative AI Service is not offered in every OCI commercial region. See the column below titled “Destination Region:” for a list of OCI commercial regions offering the OCI Generative AI Service. There are OCI services which document (in the “Oracle PaaS and IaaS Universal Credits Service Descriptions”) that they make calls into the OCI Generative AI Service. For a given call into the OCI Generative AI Service, if the Calling Region and Destination Region are not the same, then a cross-region call will be made. Area Calling Regions:OCI regions from which (cross-region) calls can be made to the OCI Generative AI Service Destination Region:OCI region offering OCI Generative AI Service Brazil GRU, VCP GRU EU AMS, MRS, LIN, ARN, CDG, MAD, ZRH, FRA FRA India HYD HYD Japan KIX, NRT KIX UAE DXB DXB UK CWL, LHR LHR U.S. PHX, IAD, SJC, ORD ORD Accessing Generative AI in the Console Sign in to the Console by using a supported browser. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI.",
    "word_count": 1130,
    "status": "success",
    "error": null
  },
  {
    "url": "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/",
    "title": "Generative AI Service | Oracle",
    "text": "Generative AI Service | Oracle Accessibility Policy  Close Search Search Oracle.com QUICK LINKS Oracle Cloud Infrastructure Oracle Fusion Cloud Applications Oracle Database Download Java Careers at Oracle Search Country   Contact Sales Sign in to Oracle Cloud Cloud Artificial Intelligence Generative AI Generative AI Service Discover the power of generative AI models equipped with advanced language comprehension for building the next generation of enterprise applications. Oracle Cloud Infrastructure (OCI) Generative AI is a fully managed service for seamlessly integrating these versatile language models into a wide range of use cases, including writing assistance, summarization, analysis, and chat. Speak to an AI expert Overview Features Pricing FAQ Product Tour Navigate the complexities of GenAI Learn how organizations prioritize GenAI use cases and accelerate adoption with key strategies for successful implementation. Access the IDC Spotlight Apply AI to your business challenges Enter a new era of productivity with generative AI solutions for your business. Visit the AI solutions hub Try Oracle GenAI and get a free trial Sign up for an OCI account and get free cloud credits to try our AI services, including generative AI. Start for free Learn about the future of GenAI: Beyond the hype curve Explore GenAI trends and OCI innovations tackling emerging business challenges. Watch the video (23:11) Choice of models for enterprise use cases Access pretrained, foundational models from our AI partners to summarize, embed, and generate text. Flexible fine-tuning Create custom models by fine-tuning base models with your own data set. Build new custom models or create new versions of existing custom models. Content moderation and controls Create endpoints for custom or pretrained models with autonomy to update, move, or delete as needed. Dedicated AI clusters Manage compute resources to fine-tune custom models or host endpoints for custom models. Clusters aren’t shared with users in other tenancies. How OCI Generative AI works A user’s input can include natural language, input/output examples, and instructions. The LLM analyzes the text input, and can generate, summarize, transform, extract information, or classify text based on the request from the user. A response is sent back to the user in whichever format was specified. This can include raw text or formatting such as bullets and numbering. At Oracle, we carefully thought through how an enterprise’s business processes could be enhanced with generative AI. We’ve created an end-to-end generative AI experience encompassing our entire stack. Greg Pavlik, SVP, Oracle Cloud Infrastructure Learn more about the future about generative AI Explore the OCI Generative AI cloud architecture Watch Chief Technical Architect Pradeep Vincent walk through the OCI Generative AI cloud architecture that brings flexible, efficient, and secure customization of AI models to real-world applications. Read the complete post Use cases for generative AI Financial services Using LLMs, financial firms can analyze news to refine investments, compose reports and summaries from financial data, generate explanations, perform risk analysis, and detect fraudulent activity. HR Create new job descriptions, screen candidates, personalize the onboarding and employee experience, generate customized career plans, and assist with performance evaluations. Retail Improve customer service with advanced conversational chatbots, write product descriptions, and automate personalized messages and rewards. Marketing Gather market analyses, automate blog writing, draft video scripts, generate logos and branded content, and get customer churn predictions. Sales Add validated company profiles, create customer profile analyses, automate answers to information requests, and produce personalized training modules. Development Generate code drafts, perform code correction and refactoring, create multiple IT architecture designs and iterate on them, and generate test cases and data. Healthcare Automate administrative tasks, improve communication speed by generating doctor discharge notes, and create personalized treatment plans. Customer support Automate case summarization and provide instantaneous, accurate answers with conversational chatbots enhanced with retrieval capabilities. Learn more about Oracle AI OCI Generative AI OCI Generative AI Agents OCI AI Services OCI Data Labeling OCI Data Science AI Vector Search Machine Learning in Oracle Database Autonomous Database Select AI HeatWave GenAI Resources for Oracle AI AI use cases ebook Workshops Tutorials and examples What Is Generative AI? How Does It Work? What Is Retrieval-Augmented Generation (RAG)? 6 Common AI Model Training Challenges Oracle and Accenture Generative AI: Unlocking Enterprise Value (PDF) AI documentation OCI Generative AI OCI Data Science Oracle Digital Assistant OCI Speech OCI Language OCI Vision OCI Document Understanding Machine Learning in Oracle Database Get started with Oracle AI Free AI trial AI hands-on lab AI solutions hub Contact sales Try Oracle AI and get a 30-day trial Oracle offers a free pricing tier for most AI services as well as a free trial account with US$300 in credits to try additional cloud services. Get the details and sign up for your free account. Try Oracle AI for free Which Oracle AI and ML services offer a free pricing tier? OCI Speech OCI Language OCI Vision OCI Document Understanding Machine Learning in Oracle Database OCI Data Labeling You also only have to pay compute and storage charges for OCI Data Science. Learn with an AI hands-on lab The best way to learn something is to try it yourself. Use our tutorials and hands-on labs with your own Oracle Cloud tenancy, at no charge for many services. See all AI and machine learning workshops and tutorials Welcome to Bistro AI: Create an AI-powered recipe generator With the help of generative AI, you'll provide a list of ingredients to a pretrained recipe generator to decide what to make. Start this lab for Welcome to Bistro AI Speed up data science with the Accelerated Data Science SDK Explore concepts in OCI Data Science to improve workflow efficiency and become more productive. Start this lab to Speed up data science with the Accelerated Data Science SDK Build the perfect digital assistant for your business Explore how to work with multiple chatbots and digital assistants in Oracle Digital Assistant. Start this lab to Build the perfect digital assistant for your business The beginner’s guide to building custom language AI models Discover how to automate sophisticated text analysis at scale without machine learning expertise. Start this lab for The beginner’s guide to building custom language AI models See how to apply AI today Enter a new era of productivity with generative AI solutions for your business. Learn how Oracle helps customers leverage AI embedded across the full technology stack. Visit the AI solutions hub What can you achieve with Oracle AI? Fine-tune LLMs in OCI Automate invoice processing Build a chatbot with RAG Summarize web content with generative AI And so much more! Contact sales Interested in learning more about Oracle AI? Let one of our experts help. Get in touch They can answer questions such as How can I get started with AI and machine learning? What can I do for my business with generative AI? Which services are right for me?",
    "word_count": 1127,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/getting-started.htm#get-started",
    "title": "Getting Started with Generative AI",
    "text": "Getting Started with Generative AI To get started, review the steps for the different tasks that you can perform in OCI Generative AI. Chat Get access to Generative AI resources. Use the chat models. (Optional) Integrate the results of the chat into your applications. Generate text Get access to Generative AI resources. Use the generation models. (Optional) Integrate the results of the model into your applications. Summarize text Get access to Generative AI resources. Use the summarization model.. (Optional) Integrate the results of the model into your applications. Generate embeddings for applications Get access to Generative AI resources. Use the embedding model.. (Optional) Integrate the results of the model into your applications. Integrate a custom model that's fine-tuned with your data into your applications Get access to Generative AI resources. Upload training data to an Object Storage bucket. Create a fine-tuning dedicated AI cluster. Create a custom model. Create a hosting dedicated AI cluster. Create an endpoint on the hosting dedicated AI cluster. Prompt and test the custom model. Integrate the custom model into your applications.",
    "word_count": 176,
    "status": "success",
    "error": null
  },
  {
    "url": "https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/features/",
    "title": "Generative AI Service Features | Oracle",
    "text": "Generative AI Service Features | Oracle Accessibility Policy  Close Search Search Oracle.com QUICK LINKS Oracle Cloud Infrastructure Oracle Fusion Cloud Applications Oracle Database Download Java Careers at Oracle Search Country   Contact Sales Sign in to Oracle Cloud Cloud Artificial Intelligence Generative AI Generative AI Service Generative AI Service Features Speak to an AI expert Overview Features Pricing FAQ Product Tour Key features Models Dedicated AI clusters Chat API and Playground LangChain integration LlamaIndex integration Generative AI operations OCI Generative AI for Oracle Cloud Applications Models OCI Generative AI provides access to pretrained, foundational models from Cohere and Meta. See our product documentation for the latest available models. Dedicated AI clusters With dedicated AI clusters, you can host foundational models on dedicated GPUs that are private to you. These clusters provide stable, high-throughput performance thatâ€™s required for production use cases and can support hosting and fine-tuning workloads. OCI Generative AI enables you to scale out your cluster with zero downtime to handle changes in volume. Chat API and Playground The chat experience provides an out-of-the box interface with Cohere and Meta models where users can ask questions and get conversational responses via the OCI console or API. LangChain integration OCI Generative AI is integrated with LangChain, an open source framework that can be used to develop new interfaces for generative AI applications based on language models. LangChain makes it easy to swap out abstractions and components that are necessary to work with language models. LlamaIndex integration Use LlamaIndex, an open source framework for building context-augmented applications, with OCI Generative AI to easily build RAG solutions or agents. Bring your solutions from prototype to production with custom data sources and flexible tooling. Generative AI operations OCI Generative AI provides content moderation controls, and coming soon: endpoint model swap with zero downtime, and endpoints deactivation and activation capabilities. For each model endpoint, OCI Generative AI captures a series of analytics, including call statistics, tokens processed, and error counts. OCI Generative AI for Oracle Fusion Cloud Applications By embedding features created with OCI Generative AI directly into Oracle Cloud Applications, we make it easy for customers to instantly access them without complex integrations. Learn more",
    "word_count": 361,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-embed.htm#playground-embed",
    "title": "Create Text Embeddings in Generative AI",
    "text": "Create Text Embeddings in Generative AI Use the cohere.embed models in OCI Generative AI to convert text to vector embeddings to use in applications for semantic searches, text classification, or text clustering. ConsoleCLIAPI In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select a compartment that you have permission to work in. If you don't see the playground, ask an administrator to give you access to Generative AI resources and then return to the following steps. Select Playground. Select Embedding. Select a model for creating text embeddings by performing one of the following actions: In the Model list, select a model. Select View model details, and then Select Choose model. (Optional) To use an example from the Example list, use the following steps: Select an example from the Example list. Select Run to generate embeddings for the example. Review a two-dimensional version of the output in the Output vector projection section. To visualize the output with embeddings, output vectors are projected into two dimensions and plotted as points. Points that are close together correspond to phrases that the model considers similar. Select Clear to remove all the sentences and start generating embeddings for new sentences. (Optional) Add a .png or .jpg image with a size of 5 MB or less. Only one image is allowed. In the Sentence input area, enter text in one of the following ways: Type a sentence in the 1. box, and then Select Add sentence to add more sentences. Select Upload file and select a file with text that you want to add. Note Only files with a .txt extension are allowed. Each input sentence, phrase, or paragraph must be separated with a newline character. A maximum 96 inputs are allowed for each run, and each input must be less than 512 tokens. You can add sentences manually or upload more than one file until you reach the maximum number of inputs. For the Truncate parameter, select whether to truncate the start or end tokens when the tokens exceed the maximum number of allowed tokens (512). Tip For input that exceeds 512 tokens, if you set the Truncate parameter to None, you get an error message. Before you run an embedding model, select Start or End for the Truncate parameter. Select Run. Review a two-dimensional version of the output in the Output vector projection section. To visualize the outputs with embeddings, output vectors are projected into two dimensions and plotted as points. Points that are close together correspond to phrases that the model considers similar. When you're happy with the result, Select Export embeddings to JSON to get a JSON file that contains a 1024-dimensional vector for each input. (Optional) Select View code, select a programming language or framework, Select Copy code, and paste the code into a file. Ensure that the file maintains the format of the pasted code. Tip If you're using the code in an application: Ensure that you authenticate your code. Review LlamaIndex Integration and LangChain Integration. (Optional) Select Clear to remove all the sentences and start generating embeddings for new sentences. Note When you Select Clear the Truncate parameter resets to its default value of None. To create embeddings for text, use the embed-text-result operation. Enter the following command for a list of options to create text embeddings. oci generative-ai-inference embed-text-result embed-text -h For a complete list of parameters and values for the OCIGenerative AI CLI commands, see Generative AI Inference CLI and Generative AI Management CLI. Run the EmbedText operation to create text embeddings. For information about using the API and signing requests, see REST API documentation and Security Credentials. For information about SDKs, see SDKs and the CLI.",
    "word_count": 643,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground.htm#use-playground",
    "title": "Using the Large Language Models (LLMs) in Generative AI",
    "text": "Using the Large Language Models (LLMs) in Generative AI The playground is an interface in the Oracle Cloud Console for exploring the hosted pretrained and custom models in OCI Generative AI without writing a single line of code. Use the playground to test your use cases and refine prompts and parameters. When you're happy with the results, integrate the generated code into your applications. In addition to the playground, you can chat, generate text, summarize, or generate embeddings using the Generative AI Inference API operations and the Generative AI Inference CLI commands. You can perform the following tasks with the OCI Generative AI large language models (LLMs): Chat: Ask questions and get conversational responses through an AI chatbot. Generate text: Run prompts to generate or classify text or extract information from text. Summarize: Use as an executive summary generator for documents that are too long to read and summarize any type of text into free-form paragraphs or bullet points and optionally specify the tone. Generate embeddings for text: Provide sentences and phrases and have the model group those sentences based on their meaning. Then use the generated embeddings in applications.",
    "word_count": 190,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/use-playground-chat.htm#chat",
    "title": "Chat in OCI Generative AI",
    "text": "Chat in OCI Generative AI Use the provided large language chat models in OCI Generative AI to ask questions and get conversational responses through an AI chatbot. ConsoleCLIAPI In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select a compartment that you have permission to work in. If you don't see the playground, ask an administrator to give you access to Generative AI resources and then return to the following steps. Select Playground. Select Chat. Select a model for the chat experience by performing one of the following actions: In the Model list, select a pretrained model such as meta.llama-3.1-70b-instruct, cohere.command-r-08-2024, cohere.command-r-plus-08-2024, or a custom model. The custom models are displayed as model name (endpoint name). Select View model details, select a model and then select Choose model. Note The meta.llama-3.1-405b-instruct model isn't available for on-demand access in all regions. To access this model, perform one of the following options: Set up dedicated access: Switch to a a region supported for dedicated clusters for the meta.llama-3.1-405b-instruct chat model. Then, create a a hosting cluster and an endpoint for this model. Switch to an on-demand region: Switch to the US Midwest (Chicago) region that's supported for on-demand inferencing for the meta.llama-3.1-405b-instruct chat model. The meta.llama-3.2-11b-vision-instruct model isn't available for on-demand access in any region. To access this model, perform the following option: Set up dedicated access: Switch to a a region supported for dedicated clusters for the meta.llama-3.2-11b-vision-instruct chat model. Then, create a a hosting cluster and an endpoint for this model. Learn about costs and model retirements for on-demand and dedicated serving modes. Start a conversation by typing a prompt or selecting an example from the Example list to use as a base prompt or to learn from. For models that accept images such as meta.llama-3.2-90b-vision-instruct, you can add a .png or .jpg image with a size of 5 MB or less. (Optional) Set new values for the parameters. For parameter details, see the parameter definitions select the model that you're using, listed in Chat Models. Select Submit. Enter a new prompt or to continue the chat conversation, enter a follow-up prompt and select Submit. Important In the playground, for the vision models that accept images: Submitting an image without a prompt doesn't work. When you submit an image, you must submit a prompt about that image in the same request. You can then submit follow-up prompts and the model keeps the context of the conversation. To add the next image and text, you must clear the chat which results in losing context of the previous conversation because of clearing the chat. (Optional) To change the responses, select Clear chat, update the prompts and parameters, and select Submit. Repeat this step until you're happy with the output. (Optional) To copy the code that generated the output, select View code, select a programming language or a framework, select Copy code, paste the code into a file and save the file. Ensure that the file maintains the format of the pasted code. Tip If you're using the code in an application: Ensure that you authenticate your code. Review LlamaIndex Integration and LangChain Integration. (Optional) To start a new conversation, select Clear chat. Note When you select Clear chat, the chat conversation is erased, but the model parameters remain unchanged, and you can continue using the last settings you applied. If you switch to a different feature, such as Generation, and then return to the Chat playground, both the chat conversation and model parameters reset to their default values. Learn about Cohere chat parameters. To chat, use the chat-result operation in Generative AI Inference CLI. Enter the following command for a list of options to use with the chat-result operation. oci generative-ai-inference chat-result -h For a complete list of parameters and values for the OCIGenerative AI CLI commands, see Generative AI Inference CLI and Generative AI Management CLI. Run the Chat operation to chat using the large language models. For information about using the API and signing requests, see REST API documentation and Security Credentials. For information about SDKs, see SDKs and the CLI.",
    "word_count": 714,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/deprecating.htm#retired-models",
    "title": "Retiring the Models",
    "text": "Retiring the Models OCI Generative AI retires its large language models (LLMs) based on each model's type and serving mode. The LLMs serves the user requests in either an on-demand mode or a dedicated mode. Review the following topics to learn about deprecation and removal times and to decide which serving mode works best for you. About model retirement and serving modes Model retirement dates (on-demand mode) Model retirement dates (dedicated mode) Getting notified before a model retires",
    "word_count": 78,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/pretrained-models.htm#pretrained-models",
    "title": "Pretrained Foundational Models in Generative AI",
    "text": "Pretrained Foundational Models in Generative AI OCI Generative AI offers the following pretrained foundational models. Review the key features, regions, on-demand and dedicated AI cluster offerings, deprecation and retirement dates, and benchmarks for the models. Chat Models Ask questions and get conversational responses through an AI chat interface using the following chat models. Expand each section for the list of offered models in n OCI Generative AI. Cohere Models Cohere Command A (New) Cohere Command R (08-2024) Cohere Command R+ (08-2024) Cohere Command R (Deprecated) Cohere Command R+ (Deprecated) Meta Models Meta Llama 4 Maverick (New) Meta Llama 4 Scout (New) Meta Llama 3.3 (70B) Meta Llama 3.2 90B Vision Meta Llama 3.2 11B Vision Meta Llama 3.1 (405B) Meta Llama 3.1 (70B) Meta Llama 3 (70B) xAI Models xAI Grok 3 xAI Grok 3 Mini xAI Grok 3 Fast xAI Grok 3 Mini Fast Embed Models Cohere Embed 4 (New) Cohere Embed English Image 3 Cohere Embed English Light Image 3 Cohere Embed Multilingual Image 3 Cohere Embed Multilingual Light Image 3 Cohere Embed English 3 Cohere Embed English Light 3 Cohere Embed Multilingual 3 Cohere Embed Multilingual Light 3 Rerank Model Cohere Rerank 3.5 (New) Generation and Summarization Models (Deprecated) Cohere Command (52B) Cohere Command Light",
    "word_count": 208,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/llama-index.htm#llama-index",
    "title": "LlamaIndex Integration",
    "text": "LlamaIndex Integration As a provider of large language models (LLMs), Generative AI service has an integration with LlamaIndex. OCI Generative AI has a LlamaIndex integration that's supported for Python. Using the OCI Generative AI service, you can access ready-to-use pretrained models, or create and host your own fine-tuned custom models based on your own data on dedicated AI clusters. References About LlamaIndex integration with OCI Generative AI Using OCI Generative AI chat models with LlamaIndex Using OCI Generative AI embedding models with LlamaIndex Tip When you chat or create text embeddings in the playground, OCI Generative AI creates code samples for LlamaIndex that include your prompts and embeddings. You can then use these code samples in your applications. See Chat in the Console and Creating text embeddings in the Console.",
    "word_count": 130,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/GSG/Tasks/contactingsupport.htm",
    "title": "Support Requests",
    "text": "Support Requests Visit the Support Center to browse and create support requests for technical and billing requests, including service limit increases. To visit the Support Center, list support requests. Note To get help signing in, or to contact support when not signed in, see Troubleshooting Signing In to the Console. Support requests are available to paid accounts only. Customers using only Always Free resources and customers using Free Tier accounts aren't eligible for Oracle Support. If you need support, use Support Chat and Cloud Customer Connect. If the available resources didn't resolve your issue and you need to talk to someone, then you can create a support request. In addition to support for technical issues, you can open support requests if you need to: Reset the password or unlock the account for the tenancy administrator Add or change a tenancy administrator Ask a question about billing and payments Request a service limit increase Request a root cause analysis (RCA) Note Support requests (incidents) don't have OCIDs . Open the Help  and then select Create a Support request. For more guidance, see Creating a Support Request. The first time you open a support request, you're automatically taken through a series of steps to provision your support account. To make changes, or if you run into problems, then see Configuring Your Oracle Support Account. You can perform the following support request tasks: Enable and validate a user to submit support requests. List the OCI products that you can create technical support requests for. List the limits and current usage of your OCI services. List, create, view details, and update your Oracle support requests. Configure your Oracle support account. Locate your OCI IDs.",
    "word_count": 280,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/langchain.htm#langchain",
    "title": "LangChain Integration",
    "text": "LangChain Integration As a provider of large language models (LLMs), Generative AI service has an integration with LangChain. LangChain is an open source modular framework for building applications that are powered by large language models (LLMs). LangChain integrations are LangChain adapters to 3rd party services. LangChain acts as a bridge, connecting the LLMs with other data sources and tools, streamlining the development of applications. LangChain for Python Applications OCI Generative AI LangChain integration is supported for Python. For set up instructions and examples, see LangChain integration. LangChain4j for Java Applications The LangChain4j library is the Java version of LangChain, created by the community for Java developers. It brings the same benefits and tools as LangChain, making it easier for Java users to work with LLMs. For APIs and examples, see LangChain4j for OCI Generative AI and LangChain4j on GitHub. References LangChain Get Started with LangChain LangChain on GitHub LangChain Providers Oracle Cloud Infrastructure Generative AI LangChain Integration LangChain4j for OCI Generative AI LangChain4j on GitHub",
    "word_count": 165,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/tools/oci-cli/3.62.1/oci_cli_docs/cmdref/generative-ai-inference.html",
    "title": "generative-ai-inference — OCI CLI Command Reference 3.62.1 documentation",
    "text": "generative-ai-inference — OCI CLI Command Reference 3.62.1 documentation Docs » generative-ai-inference View page source generative-ai-inference¶ Description¶ OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings. Use the Generative AI service inference CLI to access your custom model endpoints, or to try the out-of-the-box models to chat, generate text, summarize, and create text embeddings. To use a Generative AI custom model for inference, you must first create an endpoint for that model. Use the [Generative AI service management CLI] to create a custom model by fine-tuning an out-of-the-box model, or a previous version of a custom model, using your own data. Fine-tune the custom model on a fine-tuning dedicated AI cluster. Then, create a hosting dedicated AI cluster with an endpoint to host your custom model. For resource management in the Generative AI service, use the [Generative AI service management CLI]. To learn more about the service, see the Generative AI documentation. Available Commands¶ apply-guardrails-result apply-guardrails apply-guardrails-guardrails-text-input chat-result chat chat-cohere-chat-request chat-dedicated-serving-mode chat-generic-chat-request chat-on-demand-serving-mode embed-text-result embed-text generate-text-result generate-text-cohere-llm-inference-request generate-text-llama-llm-inference-request rerank-text-result rerank-text rerank-text-dedicated-serving-mode rerank-text-on-demand-serving-mode summarize-text-result summarize-text",
    "word_count": 198,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/api/#/en/generative-ai-inference/20231130/",
    "title": "Oracle Cloud Infrastructure API Reference and Endpoints",
    "text": "Oracle Cloud Infrastructure API Reference and Endpoints",
    "word_count": 7,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/api/#/en/generative-ai/20231130/",
    "title": "Oracle Cloud Infrastructure API Reference and Endpoints",
    "text": "Oracle Cloud Infrastructure API Reference and Endpoints",
    "word_count": 7,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/tools/oci-cli/3.62.1/oci_cli_docs/cmdref/generative-ai.html",
    "title": "generative-ai — OCI CLI Command Reference 3.62.1 documentation",
    "text": "generative-ai — OCI CLI Command Reference 3.62.1 documentation Docs » generative-ai View page source generative-ai¶ Description¶ OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings. Use the Generative AI service management CLI to create and manage dedicated AI clusters, endpoints, custom models, and work requests in the Generative AI service. For example, create a custom model by fine-tuning an out-of-the-box model using your own data, on a fine-tuning dedicated AI cluster. Then, create a hosting dedicated AI cluster with an endpoint to host your custom model. To access your custom model endpoints, or to try the out-of-the-box models to generate text, summarize, and create text embeddings see the [Generative AI Inference CLI]. To learn more about the service, see the Generative AI documentation. Available Commands¶ dedicated-ai-cluster change-compartment create delete get update dedicated-ai-cluster-collection list-dedicated-ai-clusters endpoint change-compartment create delete get update endpoint-collection list-endpoints model change-compartment create delete get update model-collection list-models work-request get list work-request-error list work-request-log-entry list-work-request-logs",
    "word_count": 181,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/API/Concepts/sdks.htm",
    "title": "Software Development Kits and Command Line Interface",
    "text": "Software Development Kits and Command Line Interface Oracle Cloud Infrastructure provides a number of Software Development Kits (SDKs) and a Command Line Interface (CLI) to facilitate development of custom solutions. Software Development Kits (SDKs) Build and deploy apps that integrate with Oracle Cloud Infrastructure services. Each SDK provides the tools you need to develop an app, including code samples and documentation to create, test, and troubleshoot. In addition, if you want to contribute to the development of the SDKs, they are all open source and available on GitHub. SDK for Java SDK for Python SDK for TypeScript and JavaScript SDK for .NET SDK for Go SDK for Ruby Command Line Interface (CLI) The CLI provides the same core capabilities as the Oracle Cloud Infrastructure Console and provides additional commands that can extend the Console's functionality. The CLI is convenient for developers or anyone who prefers the command line to a GUI. PL/SQL SDK The Oracle Cloud Infrastructure SDK for PL/SQL enables you to write code to manage Oracle Cloud Infrastructure resources. The latest version of the PL/SQL SDK is pre-installed by Oracle for all Autonomous Database Serverless.",
    "word_count": 187,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/create-ai-cluster-fine-tuning.htm#create-ai-cluster-fine-tuning",
    "title": "Creating a Dedicated AI Cluster in Generative AI for Fine-Tuning Custom Models",
    "text": "Creating a Dedicated AI Cluster in Generative AI for Fine-Tuning Custom Models Create a dedicated AI cluster resource in OCI Generative AI to use for fine-tuning custom models. Important Some OCI Generative AI foundational pretrained base models supported for the dedicated serving mode are now deprecated and will retire no sooner than 6 months after the release of the 1st replacement model. You can host a base model, or fine-tune a base model and host the fine-tuned model on a dedicated AI cluster (dedicated serving mode) until the base model is retired. For dedicated serving mode retirement dates, see Retiring the Models. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select a compartment that you have permission to work in. Click Dedicated AI clusters. Click Create dedicated AI cluster. Select a compartment to create the dedicated AI cluster in. The default compartment is the one you selected in step 3, but you can select any compartment that you have permission to work in. (Optional) Enter a name and description for the cluster. If you don't enter a name, the system generates one that you can change later. The generated name has the format generativeaidedicatedaicluster<timestamp>. For example: generativeaidedicatedaicluster20240601202357 For Cluster type, select Fine-tuning. For Base model, select the foundational base model for the custom model that you want to fine-tune on this cluster. Important The number of units in a fine-tuning cluster is preset and depends on the foundational base model selected for fine-tuning. Fine-tuning clusters need more resources than hosting clusters because they use more GPUs. You can fine-tune several models on the same cluster, as long as the custom models train on the same foundational base model. Read the commitment unit hours for the fine-tuning cluster and select the checkbox to agree to the commitment. (Optional) Click Show advanced options and assign tags to this dedicated AI cluster. Click Create. Note Fine-tuning clusters take some time to create. After a fine-tuning cluster is in an active state, you can use it to fine-tune custom models. You can select a cluster for fine-tuning when you create a custom model. After the dedicated AI cluster is created, you can get the cluster's details.",
    "word_count": 396,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cloudshellintro.htm",
    "title": "Cloud Shell",
    "text": "Cloud Shell Oracle Cloud Infrastructure (OCI) Cloud Shell is a web browser-based terminal accessible from the Oracle Cloud Console. Oracle Cloud Infrastructure (OCI) Cloud Shell is a web browser-based terminal accessible from the Oracle Cloud Console. Cloud Shell is free to use (within monthly tenancy limits), and provides access to a Linux shell, with a pre-authenticated Oracle Cloud Infrastructure CLI, a pre-authenticated Ansible installation, and other useful tools for following Oracle Cloud Infrastructure service tutorials and labs. Note Cloud Shell provides access to the public internet only if appropriate security policies are in place. See Cloud Shell Networking for more information.Cloud Shell is a feature available to all OCI users, accessible from the Console. Your Cloud Shell will appear in the Oracle Cloud Console as a persistent frame of the Console, and will stay active as you navigate to different pages of the Console. Cloud Shell provides: An ephemeral machine to use as a host for a Linux shell, pre-configured with the latest version of the OCI Command Line Interface (CLI) and a number of useful tools 5GB of encrypted persistent storage for your home directory A persistent frame of the Console which stays active as you navigate to different pages of the console How Cloud Shell Works The Cloud Shell machine is a small virtual machine running a Bash shell which you access through the OCI Console. Cloud Shell comes with a pre-authenticated OCI CLI, set to the Console tenancy home page region, as well as up-to-date tools and utilities. Note Cloud Shell provides access to the public internet only if appropriate security policies are in place. See Cloud Shell Networking for more information. Cloud Shell comes with 5GB of encrypted persistent storage for the home directory, so you can make local changes to your home directory, and then continue working on your project when you come back to Cloud Shell. Cloud Shell is free to use (within your tenancy's monthly limits) and doesn't require any setup or prerequisites other than an IAM policy granting access to Cloud Shell. Your Cloud Shell includes a VM provisioned for you that executes in its own tenancy (so it doesn't use any of your tenancy's resources) and hosts your shell in an Oracle Linux OS while you're actively using Cloud Shell. What's Included With Cloud Shell Cloud Shell runs on Oracle Linux 8. In addition to the OCI CLI, the Cloud Shell VM running Oracle Linux 8 comes with current versions of many useful tools and utilities, including: Git Java Note Cloud Shell now enables you to install an Oracle Java release by using the Toolkit for Java Download. For more information, see Toolkit for Java Download. Python Note Multiple versions of Python are included in Cloud Shell. Use csruntimectl to switch Python versions. For example: run \"csruntimectl python list\" to get a list of available Python version, and \"csruntimectl python set <python-alias>\" to switch the Python version. Oracle GraalVM JDK 17 and Native Image Most OCI SDKs, including: Java Python Go TypeScript and JavaScript SQLcl kubectl helm maven terraform ansible nodes podman Note Docker Engine is no longer installed on Oracle Linux 8-based Cloud Shell. Podman provides a similar container management experience with a daemonless architecture. For backward compatibility, Cloud Shell provides a \"docker\" alias which is a wrapper of the command \"podman\". You can use either the \"podman\" or \"docker\" command to run containers with commands largely compatible with Docker. For more information on podman, see the Oracle Linux Podman User's Guide.When you run the podman login or docker login command to sign in to a container registry, the auth.json file is written at the following location, /run/user/<uid>/containers/auth.json for example, /run/user/1101/containers/auth.json. buildah skopeo runc iputils jqmake tmux vim NPM wget zip/unzip nano emacs pip bash sh tar nvm mysql-community-client mysqlsh ipython OCI-powershell-modules (x86_64 only) GoldenGate Admin client (x86_64 only) Fn Project CLIRequired to work with OCI Functions. Note When using the Fn CLI in Cloud Shell, OCI Functions doesn't support the creation and deployment of functions based on multi-architecture images and doesn't support applications with multi-architecture shapes. The architecture of the Cloud Shell session must match the architecture of the application. Required IAM Policy To get started with Cloud Shell, you'll need to grant user access to Cloud Shell via an IAM policy. Each service in Oracle Cloud Infrastructure integrates with IAM for authentication and authorization, for all interfaces (the Console, SDK or CLI, and REST API). Note To allow users to access the Cloud Shell managed Public Network, you'll need to grant user access via an Identity policy. For more information, see Cloud Shell Public Network. To use Oracle Cloud Infrastructure, you must be given the required type of access in a policy written by an administrator in the tenancy's root compartment, whether you're using the Console or the REST API with an SDK, CLI, or other tool. If you try to perform an action and get a message that you don't have permission or are unauthorized, confirm with your administrator that you have been granted access. Note Cloud Shell does not support policies at the compartment level, only at the tenancy level.The resource name for Cloud Shell is `cloud-shell`. The following is an example policy to grant access to Cloud Shell: allow group <GROUP-NAME> to use cloud-shell in tenancy This example policy shows how to allow a group within a domain to use Cloud Shell:allow group <DOMAIN-NAME>/<GROUP-NAME> to use cloud-shell in tenancy If you're new to policies, see Managing Identity Domains and Common Policies and Creating a Policy and Dynamic Group. Cloud Shell Limitations Keep the following limitations in mind when using Cloud Shell: Docker Engine is no longer installed on Oracle Linux 8-based Cloud Shell. Podman provides a similar container management experience with a daemonless architecture. For backward compatibility, Cloud Shell provides a \"docker\" alias which is a wrapper of the command \"podman\". You can use either the \"podman\" or \"docker\" command to run containers with commands largely compatible with Docker. For more information on podman, see the Oracle Linux Podman User's Guide. By default, Cloud Shell limits network access to OCI internal resources in your tenancy home region only unless you have enabled the Cloud Shell Public Network. Your administrator must configure an Identity policy to enable Cloud Shell Public Network. For more information, see Cloud Shell Networking. Cloud Shell comes with 5GB of storage for the VM's home directory. This storage is persistent from session to session, but after 6-months of non-use, the administrator for your tenancy will receive a notification that the storage will be removed in 60 days. Starting a Cloud Shell session resets the storage removal timer. Cloud Shell does not support mounting additional storage. Cloud Shell does not scan user files for malware or viruses. Cloud Shell sessions do not allow for any incoming connections, and there is no public IP address available. The OCI CLI will execute commands against the region selected in the Console's Region selection  when the Cloud Shell was started. Changing the region selection in the Console will not change the region for existing Cloud Shell instances; you will need to open a new Cloud Shell instance to change regions. Cloud Shell sessions have a maximum length of 24 hours, and time out after 60 minutes of inactivity. Cloud Shell uses websockets to communicate between your browser and the service. If your browser has websockets disabled or uses a corporate proxy that has websockets disabled you will see an error message (\"An unexpected error occurred\") when attempting to start Cloud Shell from the Console. Cloud Shell is designed for interactive use with Oracle Cloud Infrastructure resources. Users who need additional storage for Cloud Shell or want to run non-interactive long-running tasks are encouraged to use Compute and Storage resources in their tenancy. For maximum compatibility, Cloud Shell includes Python version 2 and Python version 3. Python 2 is the default that will run when you enter 'python' at the command line. To run Python 3, enter 'python3' at the command line. The following reserved words can't be used as the user name for a Cloud Shell user: oci, root, bin, daemon, adm, lp, sync, shutdown, halt, mai, operator, games, ftp, nobody, oci, systemd-network, dbus, polkitd, tss, and apache. Attempting to create a Cloud Shell session when logged in with a user name (or the part of the name before the @ sign if the user name is an email address) that's one of these reserved words will result in an \"Unexpected Error\" message. Entirely numerical user names (for example, \"1234\") aren't supported by Cloud Shell. The Cloud Shell session time zone is UTC, and can't be changed. In a Cloud Shell terminal, root access and sudo aren't allowed, so packages requiring root privileges can't be installed. However, many packages offer versions that don't need root permissions. These can be unpacked and installed directly in the home directory. In a Cloud Shell terminal, ping command isn't allowed. Cloud Shell boots in FIPS mode, which might affect the behavior of some commands. Cloud Shell can't generate PKCS#1 keys when using the openssl command, because Cloud Shell boots in FIPS mode. FIPS mode requires that Cloud Shell generates PKCS#8 keys. Network Sources aren't supported for the Cloud Shell Service. For more information on Cloud Shell limits, see the Cloud Shell section in Service Limits. Cloud Shell Access and Other Restrictions You can access OCI resources from Cloud Shell according to the policies granted by your tenancy administrator. There is no additional access because you're using Cloud Shell, and Cloud Shell doesn't provide any additional access to the tenancy, or private resources in your tenancy VCNs. Note Cloud Shell uses websockets to communicate between your browser and the service. If your browser has websockets disabled or uses a corporate proxy that has websockets disabled you will see an error message (\"An unexpected error occurred\") when attempting to start Cloud Shell from the Console. While Cloud Shell provides access to the internet, there is no ingress from the outside world into Cloud Shell (for example: you can't ssh in to Cloud Shell) and no public IP address available. If your tenancy admin doesn't want to enable access to the internet from OCI, they should not grant access to Cloud Shell with an IAM policy. Cloud Shell Resource Location and Ownership When you first start Cloud Shell, the service creates a persistent block storage volume (5GB) for your home directory. The home directory volume is located in your tenancy home region. The machine running your Cloud Shell session is also located in your tenancy home region. Note Cloud Shell uses your user OCID to create your home directory. If you have multiple accounts in a tenancy (for example, you have a federated and a non-federated user account), you will get a separate, unique Cloud Shell home directory for each account. Changing the Console region selection, or logging in to the Console via a different regional URL will have no effect on where your Cloud Shell machine and home directory volume are located. To confirm your tenancy home region, view your Tenancy Details page in the Console. Note Cloud Shell resources (including the VM used for your Cloud Shell session) are owned by the Cloud Shell service and do not exist in your tenancy. Because of this, you cannot add the Cloud Shell VM you are using to a dynamic group in your tenancy, or use the instance principle of the instance used for your Cloud Shell session. Cloud Shell and Regions When you start Cloud Shell, the service configures your Cloud Shell session with the currently selected region in the Console so that the OCI CLI is interacting with the selected Console region. In the default bash prompt in Cloud Shell, the region that the OCI CLI is interacting with is echoed in the Cloud Shell command line prompt: Any changes to the selected region in Console after you've started your Cloud Shell session will not have an effect on your active Cloud Shell session.If you want to change the region that the OCI CLI is interacting with, in Cloud Shell, you can either: Exit your current Cloud Shell session, then change the selected region in the Console, then start a new Cloud Shell session. Modify the currently selected OCI CLI profile via the OCI_CLI_PROFILE environment variable For more information, see the \"Managing Regions\" section in Using Cloud Shell. Cloud Shell Architecture If you are a paid tier user, you can choose a default architecture (ARM (aarch64), x86_64 or No Preference) for your Cloud Shell sessions. By default, the architecture preference is set to No Preference. When this is selected, your Cloud Shell sessions will be based on either the x86_64 or ARM (aarch64) architecture, depending on the hardware available in the region. Selecting an architecture To select an architecture: Open the Actions , which is accessible from within Cloud Shell or Code Editor, and choose Architecture. This will display the Architecure dialog: The Architecture dialog shows the currently selected architecture. To select your preferred architecture, select the appropriate radio button and then click the Confirm and Restart button. Note If a region does not support a particular architecture, you will not be able to choose that architecture. Note Before switching your Cloud Shell architecture, ensure that your tools and workloads are compatible with the architecture you are about to choose. After a successful architecture migration, you will see this notification:",
    "word_count": 2238,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/create-new-model.htm#create-new-model",
    "title": "Creating a Custom Model in Generative AI",
    "text": "Creating a Custom Model in Generative AI Create a custom model in OCI Generative AI by fine-tuning a base model with your own dataset. Before you create a model, ensure that you have permission to create custom models and list buckets, and that your dataset meets the training data requirements. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select a compartment to create the custom model in. Click Custom models. Click Create model. Select Create a new model. Select a compartment to create the model in. The default compartment is the one that you selected in step 3, but you can select any compartment that you have permission to work in. (Optional) Enter a name for the custom model. Start the name with a letter or underscore, followed by letters, numbers, hyphens, or underscores. The length can be 1–255 characters. If you don't enter a name, the system generates a name that you can change later. The generated name has the format generativeaimodel<timestamp>. For example: generativeaimodel20240531234930 (Optional) Enter a version for the model. If you don't enter a version, the system generates a version that you can change later. The generated version has the format v<timestamp>. For example: v20240531234930 (Optional) Enter a description for the model. Click Next. Select a Base Model for the custom model. For details about the base model, see the key features of the pretrained models. Select a Fine-tuning method. Important For guidelines, see Choosing a Fine-Tuning Method in Generative AI. Select a fine-tuning dedicated AI cluster by performing one of the following actions: Select a Dedicated AI cluster from the drop-down list. If you just created a cluster, wait for that cluster to become active. Ensure that the base model that's associated with this cluster matches the base model in step 12. In the Dedicated AI cluster drop-down list, click Create new dedicated AI cluster and perform the following steps: (Optional) Enter a name and description. Select a Base model that matches the base model in step 12. Read the commitment unit hours for the fine-tuning dedicated AI cluster and select the checkbox to agree to the commitment. Click Create and wait for the cluster to become active. From the Dedicated AI cluster drop-down list, click the dedicated AI cluster that you created. (Optional) Under Advanced options, click Show hyperparameters, review the tooltips and hint text and update the values as needed. Also see Hyperparameters for Fine-Tuning a Model in Generative AI. To reset the values, click Restore defaults. Click Next. Select the Object Storage bucket that contains the training dataset. If the bucket isn't listed, perform the following actions: Ensure that the bucket is in the same region as the custom model. Click Change compartment and select the compartment that hosts the bucket with the training data. Ask an administrator to give you permission to buckets and objects in this compartment. Select the Training file to use for this model. Preview how the data in the training file is ingested. If the data records parse correctly, click Submit. Otherwise, fix the data. After the model is created, you can get the custom model's details from the detail page. Note Custom models aren't available in all regions. See Pretrained Foundational Models in Generative AI for the regions that a custom model is available in. For example, a custom text generation and summarization model with the base model of cohere.command, is only available in US Midwest (Chicago).",
    "word_count": 605,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/performance.htm#performance",
    "title": "Dedicated AI Cluster Performance Benchmarks in Generative AI",
    "text": "Dedicated AI Cluster Performance Benchmarks in Generative AI Review the inference speed, latency, and throughput in several scenarios when one or more concurrent users call large language models hosted on dedicated AI clusters in OCI Generative AI. The benchmarks are provided for models in the following families: Cohere models Meta models The following metrics are used for the benchmarks. For metric definitions, see About the Metrics. Benchmark Metrics Metric Unit Token-level inference speed tokens per second (TPS) Token-level throughput tokens per second (TPS) Request-level latency seconds Request-level throughput requests per minute (RPM) or requests per second (RPS) About the Metrics Review the definitions for the following benchmark metrics. Metric 1: Token-level inference speed This metric is defined as the number of output tokens generated per unit of end-to-end latency. For applications where matching the average human reading speed is required, users should focus on scenarios where the speed is 5 tokens/second or more, which is the average human reading speed. In other scenarios requiring faster near real-time token generation, such as 15 tokens/second inference speed, for example in dialog and chat scenarios where the number of concurrent users that could be served is lower, and the overall throughput is lower. Metric 2: Token-level throughput This metric quantifies the average total number of tokens generated by the server across all simultaneous user requests. It provides an aggregate measure of server capacity and efficiency to serve requests across users. When inference speed is less critical, such as in offline batch processing tasks, the focus should be where throughput peaks and therefore server cost efficiency is highest. This indicates the LLM's capacity to handle a high number of concurrent requests, ideal for batch processing or background tasks where immediate response is not essential. Note: The token-level throughput benchmark was done using the LLMPerf tool. The throughput computation has an issue where it includes the time it requires to encode the generated text for token computation. Metric 3: Request-level latency This metric represents the average time elapsed between the request submission and the time it took to complete the request, such as after the last token of the request was generated. Metric 4: Request-level throughput The number of requests served per unit time, either per minute or per second. Concurrency Number of users that make requests at the same time. Important The performance (inference speed, throughput, latency) of a hosting dedicated AI cluster depends on the traffic scenarios going through the model that it's hosting. Traffic scenarios depend on: The number of concurrent requests. The number of tokens in the prompt. The number of tokens in the response. The variance of (2) and (3) across requests.",
    "word_count": 442,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/create-ai-cluster-hosting.htm#create-ai-cluster-hosting",
    "title": "Creating a Dedicated AI Cluster in Generative AI for Hosting Models",
    "text": "Creating a Dedicated AI Cluster in Generative AI for Hosting Models Create a dedicated AI cluster resource in OCI Generative AI to host endpoints for pretrained base models and custom models. Important Not Available on-demand: All OCI Generative AI foundational pretrained models supported for the on-demand serving mode that use the text generation and summarization APIs (including the playground) are now retired. We recommend that you use the chat models instead. Can be hosted on clusters: If you host a summarization or a generation model such as cohere.command on a dedicated AI cluster, (dedicated serving mode), you can continue to use that model until it's retired. These models, when hosted on a dedicated AI cluster are only available in US Midwest (Chicago). See Retiring the Models for retirement dates and definitions. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select a compartment in which you want to host the models. Ensure that you have permission to use or manage generative-ai-family and object-family resources in this compartment. In the left , select a compartment that you have permission to work in. Click Dedicated AI clusters. Click Create dedicated AI cluster. Select a compartment to create the dedicated AI cluster in. The default compartment is the one you selected in step 3, but you can select any compartment that you have permission to work in. (Optional) Enter a name and description for the cluster. If you don't enter a name, the system generates one that you can change later. The generated name has the format generativeaidedicatedaicluster<timestamp>. For example: generativeaidedicatedaicluster20240601202357 For Cluster type, click Hosting. For Base model, select the foundational base model for the models that you want to host on this cluster. The model list includes only the base models that are not yet retired for the on-demand mode. For dedicated cluster unit sizes, see Matching Base Models to Clusters For rules about creating endpoints for the models hosted on clusters, see Adding Endpoints to Hosting Clusters. (Optional) Increase the number of instances in the Model replica field. Important When you create a cluster for hosting models for inference, by default one unit is created for the base model that you select. To increase the throughput, you can increase the number of instances in the Model replica field now, or later when you edit the cluster. For example, creating two model replicas on this cluster, requires two units. Read the commitment unit hours for the hosting cluster and select the checkbox to agree to the commitment. (Optional) Click Show advanced options and assign tags to this cluster. Click Create. Note Clusters take a few minutes to create. After the cluster is in an active state, you can select that cluster to host a model, when you create an endpoint for that model.",
    "word_count": 495,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/create-endpoint.htm#create-endpoint",
    "title": "Creating an Endpoint in Generative AI",
    "text": "Creating an Endpoint in Generative AI Create an endpoint for a custom or pretrained model on a hosting dedicated AI cluster in OCI Generative AI. For rules about creating endpoints for the models hosted on clusters, see Adding Endpoints to Hosting Clusters. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select the compartment that contains the custom model that you want to add an endpoint to. Perform one of the following actions: To create an endpoint for a custom model with the model name and version pre-populated: Select Custom models. Select the name of the custom model that you want to add an endpoint for. Find the foundational base model for the custom model. You select the base model when you match the model to a cluster in the following steps. Under Resources, select Endpoints. Select Create endpoint. To create an endpoint for a ready-to-use pretrained foundational model or a custom model: Select Endpoints. Select Create endpoint (Optional) Enter a name for the endpoint. Start the name with a letter or underscore, followed by letters, numbers, hyphens, or underscores. The length can be 1 to 255 characters. If you don't enter a name, the system generates a name that you can change later. The generated name has the format generativeaiendpoint<timestamp>. generativeaiendpoint20240531235319 If not selected, select the model name and version that you want to add an endpoint for. Tip If the model is in a different compartment than the current compartment, select Change compartment and select the compartment that hosts the model. We recommend that you create the endpoint in the same compartment as the model. If the custom model that you're looking for isn't listed, select Cancel. Then under Generative AI, select Custom models and ensure that the custom model is in an active state. Select a hosting dedicated AI cluster by performing one of the following actions: If you already have a cluster, select a Dedicated AI cluster from the drop-down list. If you just created a cluster, wait for that cluster to become active. Ensure that the base model that 's associated with this cluster matches the base model of the custom model. To create a cluster, in the Dedicated AI cluster drop-down list, select Create new dedicated AI cluster and perform the following steps: (Optional) Enter a name and description. Select a Base model that matches the base model of the model that you want to host. Add 1 model replica to the endpoint. When you create a cluster you need at least one unit for an endpoint. For an existing cluster, you can use that same unit to host new endpoints. Each instance hosts all the active endpoints. Increasing the instance count on a cluster, increases the number of supported RPMs for all active endpoints hosted on a cluster. Read the commitment unit hours for the hosting dedicated AI cluster and select the checkbox to agree to the commitment. Select Create and wait for the cluster to become active. From the Dedicated AI cluster drop-down list, select the dedicated AI cluster that you created. Select whether to enable the following guardrails. Content moderation Off: Don't apply content moderation and output explicit content. Block: Help identify and apply content moderation. Inform: Don't apply content moderation, but aim to inform the user if the model detects content that needs moderation. Prompt injection (PI) protection Off: Don't apply PI protection and allow unrestricted input. Block: Help identify and protect against prompt injection. Inform: Don't apply PI protection, but aim to inform the user if the model detects content that needs PI protection. Personally identifiable information (PII) protection Off: Don't apply PII protection, Instead, output content without data exposure restrictions. Block: Help identify and protect PII such as help remove personal data from responses. Inform: Don't apply PII protection, but aim to inform the user if the model detects content that needs PII protection. (Optional) Select Show advanced options and assign tags to the endpoint. Select Create endpoint. You're directed to the endpoint details page where you can track the state of the endpoint. After the endpoint is active, select View in playground and start using the model from this endpoint.",
    "word_count": 726,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/integrate-models.htm#integrate",
    "title": "Integrating the Models in Generative AI",
    "text": "Integrating the Models in Generative AI To integrate the OCI Generative AI pretrained ready-to-use base models or fine-tuned custom models into applications, see the following topics. Creating applications from large language models (LLMs) using the following integration: LlamaIndex (New) LangChain Accessing the code in Generative AI and integrating the code into applications.",
    "word_count": 52,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/calculate-cost.htm#calculate-cost",
    "title": "Calculating Cost in Generative AI",
    "text": "Calculating Cost in Generative AI In OCI Generative AI, you can pay for on-demand inferencing or dedicated AI clusters: On-demand Inferencing You pay as you go. You pay for each inference call's character length. On the pricing page, when you see the price for number of transactions, that's the number of characters in inference calls. One transaction equals to one character. Dedicated AI Clusters You get a dedicated set of GPUs. You can fine-tune custom models on the dedicated AI clusters. You can host replicas of foundational and fine-tuned models on the dedicated AI clusters. You commit in advance to certain hours of using the dedicated AI clusters. For prices, see the pricing page. Review the following topics and examples to help you decide between on-demand inferencing or using dedicated AI clusters and to calculate the cost for each option. Paying for On-Demand Inferencing Paying for Dedicated AI Clusters",
    "word_count": 149,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/metric-details.htm#metric-details",
    "title": "Metric Details in Generative AI",
    "text": "Metric Details in Generative AI You can monitor OCI Generative AI resources through the metrics provided in this service. You can also use the OCI Monitoring service to create custom queries and alarms to notify you when these metrics meet alarm-specified triggers. Hosting Dedicated AI Cluster Metrics This section lists the metrics for the hosting dedicated AI clusters. The fine-tuning dedicated clusters don't display metrics. Metric Display Name Description Utilization The available capacity for a dedicated AI cluster displayed as percentage over time Total number of input Number of input tokens that the models on this hosting dedicated AI cluster have processed Total number of output Number of output tokens that the models on this hosting dedicated AI cluster have processed You can get the preceding metrics from a hosting dedicated AI cluster's detail page. Endpoint Metrics This section lists the metrics for model endpoints in Generative AI. Metric Display Name Description Total processing time Total processing time for a call to finish Number of calls Number of input tokens that the model that's hosted on this endpoint has processed Service Errors Count Number of calls with a service internal error Client Errors Count Number of calls with a client side error Total number of input Number of input tokens that the model that's hosted on this endpoint has processed Total number of output Number of output tokens that the model that's hosted on this endpoint has processed Success rate of calls Successful calls divided by the total number of calls You can get the preceding metrics from an endpoint's detail page. Metrics for Custom Queries You can create custom queries and alarms for the Generative AI cluster and endpoint metrics through the Monitoring service. This section lists the parameters that you can use to create custom queries for Generative AI metrics by using the Monitoring service. Metric Parameter Display Name Description ClientErrorCount Client Errors Count Number of calls with a client side error InputTokenCount Total number of input Number of input tokens that the models hosted on this resource have processed InvocationLatency Total processing time Total processing time for a call to finish on this resource OutputTokenCount Total number of output Number of output tokens that the models hosted on this resource have processed ServerErrorCount Service Errors Count Number of calls with a service internal error TotalInvocationCount Number of calls Number of calls For the steps on how to create these custom queries, see Creating a Query for Generative AI Metrics.",
    "word_count": 412,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/get-cluster-metrics.htm#get-cluster-metrics",
    "title": "Getting a Dedicated AI Cluster's Metrics in Generative AI",
    "text": "Getting a Dedicated AI Cluster's Metrics in Generative AI Get metrics for a hosting dedicated AI cluster in OCI Generative AI. Note You can get metrics only for the hosting dedicated AI clusters. The fine-tuning dedicated clusters don't display metrics. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select the compartment that contains the dedicated AI clusters. Click Dedicated AI clusters. Click the name of the dedicated AI cluster that you want to see metrics for. Click Cluster metrics. For details about the Cluster metrics and to use the Monitoring service to create your own charts and get notified when these metrics meet alarm-specific triggers, see Metric Details in Generative AI.",
    "word_count": 145,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/get-endpoint-metrics.htm#get-endpoint-metrics",
    "title": "Getting an Endpoint's Metrics in Generative AI",
    "text": "Getting an Endpoint's Metrics in Generative AI Get metrics for an endpoint in OCI Generative AI. In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Analytics & AI. Under AI Services, select Generative AI. Select the compartment that contains the endpoint that you want to see its metrics. Select Endpoints. Select the name of the endpoint that you want to see metrics for. Under Resources, select Endpoint metrics. For details about the Endpoint metrics and to use the Monitoring service to create your own charts and get notified when these metrics meet alarm-specific triggers, see Metric Details in Generative AI.",
    "word_count": 124,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/create-query.htm#create-query",
    "title": "Creating a Query for Generative AI Metrics",
    "text": "Creating a Query for Generative AI Metrics By using the OCI Monitoring service, you can create custom queries and trigger alarms to monitor the OCI Generative AI dedicated AI clusters and endpoints. Note To get permission to work with the Monitoring service resources, ask an administrator to review the IAM policies in Securing Monitoring and grant you the proper access for your role. For permission to work with Generative AI resources, ask an administrator to give you access to Generative AI resources. Important In the Monitoring service, to add Generative AI resources to queries or alarms, select the namespace, oci_generativeai and refer to Monitoring Query Language (MQL) Reference for the query syntax. ConsoleCLIAPI In the  bar of the Console, select a region with Generative AI, for example, US Midwest (Chicago) or UK South (London). See which models are offered in your region. Open the   and select Observability & Management. Under Monitoring, select Service Metrics. Select a compartment that has the Generative AI resources that you want to monitor. For Metric namespace, select oci_generativeai. A metric namespace is the service or application that emits the metrics. The metric namespace for the Generative AI service is oci_generativeai. The Service Metrics page dynamically updates the page to show charts for the oci_generativeai metric namespace. See Viewing Default Metric Charts for a Metric Namespace (Multiple Resources). To create a query, follow the steps in Creating a Query. (Optional) To create an alarm, follow the steps in Creating a Basic Alarm. Use the oci monitoring metric-data summarize-metrics-data command and required parameters to query metric data: oci monitoring metric-data summarize-metrics-data [OPTIONS] When specifying a dimension value within the query (--query-text), surround it with double quotes, and escape each double quote with a backslash character (\\). Example: oci monitoring metric-data summarize-metrics-data --compartment-id <compartment_OCID> --namespace oci_computeagent --query-text \"CpuUtilization[1m]{resourceId = \\\"<instance_OCID>\\\"}.max()\" For a complete list of parameters and values for CLI commands, see the Command Line Reference for Monitoring. Use the oci monitoring alarm create command and required parameters to create an alarm: oci monitoring alarm create --compartment-id <compartment_OCID> --destinations <file_or_text> --display-name <name> --is-enabled <true_or_false> --metric-compartment-id <compartment_OCID> --namespace <metric_namespace> --query-text <mql_expression> --severity <level> For a complete list of parameters and values for CLI commands, see the Command Line Reference for Monitoring. Refer to the Monitoring API for using the Monitoring service APIs. For example, run the SummarizeMetricsDataDetails operation to query metric data and CreateAlarm to create an alarm. For information about using the API and signing requests, see REST API documentation and Security Credentials. For information about SDKs, see SDKs and the CLI.",
    "word_count": 424,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/cohere-models.htm",
    "title": "Cohere",
    "text": "Cohere Review the key features and regions for the following Cohere large language models available through OCI Generative AI. Chat Models Cohere Command A (New) Cohere Command R (08-2024) Cohere Command R+ (08-2024) Cohere Command R (Deprecated) Cohere Command R+ (Deprecated) Cohere Command (52B) Cohere Command Light Embedding Models Cohere Embed 4 (New) Cohere Embed English Image 3 Cohere Embed English Light Image 3 Cohere Embed Multilingual Image 3 Cohere Embed Multilingual Light Image 3 Cohere Embed English 3 Cohere Embed English Light 3 Cohere Embed Multilingual 3 Cohere Embed Multilingual Light 3 Rerank Model Cohere Rerank 3.5 (New)",
    "word_count": 99,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-models.htm",
    "title": "Meta",
    "text": "Meta Review the key features and regions for the following Meta large language models available through OCI Generative AI. Chat Models Meta Llama 4 Maverick (New) Meta Llama 4 Scout (New) Meta Llama 3.3 (70B) Meta Llama 3.2 90B Vision Meta Llama 3.2 11B Vision Meta Llama 3.1 (405B) Meta Llama 3.1 (70B) Meta Llama 3 (70B)",
    "word_count": 57,
    "status": "success",
    "error": null
  },
  {
    "url": "https://docs.oracle.com/en-us/iaas/Content/generative-ai/xai-models.htm",
    "title": "The xAI Platform for OCI Generative AI",
    "text": "The xAI Platform for OCI Generative AI With the xAI platform for OCI Generative AI, you can reach the pretrained Grok 3 and Grok 3 Mini models. Grok 3 excels at enterprise use cases such as data extraction, coding, and summarizing text, and has a deep domain knowledge in finance, healthcare, law, and science. Grok 3 Mini is a lightweight model that thinks before responding. Grok 3 Mini is fast, smart, and great for logic-based tasks that don't require deep domain knowledge. To get started, go to the Console's playground to try out the ready-to-use Grok models. Review the key features and regions for the xAI platform for OCI Generative AI. Chat Models xAI Grok 3 xAI Grok 3 Mini xAI Grok 3 Fast xAI Grok 3 Mini Fast",
    "word_count": 129,
    "status": "success",
    "error": null
  }
]